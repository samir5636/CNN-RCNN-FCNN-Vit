{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAaHjuzH8Ax0"
      },
      "source": [
        "## 1. Establish a CNN Architecture (Based on Pytorch Library) to classify MINST Dataset, by defining layers (Convolution, pooling, fully connect layer), the hyper-parameters (Kernels,Padding , stride, optimizers, regularization, etc) and running the model in GPU mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3QPqrBbc8K_Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        self.fc1 = nn.Linear(in_features=32*7*7, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32*7*7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqq_9-18iuo"
      },
      "source": [
        "## 2. Do the same thing with Faster R-CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jvF2y7fC8nbg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the architecture for the modified Faster R-CNN\n",
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten before FC\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b39jmXwZewop"
      },
      "source": [
        "## 3. Compare the two models (By using several metrics (Accuracy, F1 score, Loss, Training time))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcvgoYPZe3Hb",
        "outputId": "323fefd2-fbf3-42be-dc1f-e51f4083ebe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], CNN Loss: 0.1673, Faster R-CNN Loss: 0.1383\n",
            "Training Time - CNN: 15.98 seconds, Faster R-CNN: 15.33 seconds\n",
            "CNN Accuracy: 98.33%, CNN F1 Score: 0.9833\n",
            "Faster R-CNN Accuracy: 98.48%, Faster R-CNN F1 Score: 0.9848\n",
            "Epoch [2/10], CNN Loss: 0.0492, Faster R-CNN Loss: 0.0418\n",
            "Training Time - CNN: 15.76 seconds, Faster R-CNN: 15.60 seconds\n",
            "CNN Accuracy: 98.58%, CNN F1 Score: 0.9859\n",
            "Faster R-CNN Accuracy: 98.88%, Faster R-CNN F1 Score: 0.9888\n",
            "Epoch [3/10], CNN Loss: 0.0350, Faster R-CNN Loss: 0.0283\n",
            "Training Time - CNN: 15.58 seconds, Faster R-CNN: 15.38 seconds\n",
            "CNN Accuracy: 99.01%, CNN F1 Score: 0.9901\n",
            "Faster R-CNN Accuracy: 98.97%, Faster R-CNN F1 Score: 0.9897\n",
            "Epoch [4/10], CNN Loss: 0.0269, Faster R-CNN Loss: 0.0209\n",
            "Training Time - CNN: 15.49 seconds, Faster R-CNN: 15.39 seconds\n",
            "CNN Accuracy: 99.09%, CNN F1 Score: 0.9909\n",
            "Faster R-CNN Accuracy: 99.06%, Faster R-CNN F1 Score: 0.9906\n",
            "Epoch [5/10], CNN Loss: 0.0218, Faster R-CNN Loss: 0.0151\n",
            "Training Time - CNN: 15.50 seconds, Faster R-CNN: 15.52 seconds\n",
            "CNN Accuracy: 98.98%, CNN F1 Score: 0.9898\n",
            "Faster R-CNN Accuracy: 99.13%, Faster R-CNN F1 Score: 0.9913\n",
            "Epoch [6/10], CNN Loss: 0.0188, Faster R-CNN Loss: 0.0114\n",
            "Training Time - CNN: 15.08 seconds, Faster R-CNN: 15.86 seconds\n",
            "CNN Accuracy: 98.87%, CNN F1 Score: 0.9887\n",
            "Faster R-CNN Accuracy: 99.02%, Faster R-CNN F1 Score: 0.9902\n",
            "Epoch [7/10], CNN Loss: 0.0145, Faster R-CNN Loss: 0.0103\n",
            "Training Time - CNN: 15.05 seconds, Faster R-CNN: 15.69 seconds\n",
            "CNN Accuracy: 99.18%, CNN F1 Score: 0.9918\n",
            "Faster R-CNN Accuracy: 99.10%, Faster R-CNN F1 Score: 0.9910\n",
            "Epoch [8/10], CNN Loss: 0.0140, Faster R-CNN Loss: 0.0093\n",
            "Training Time - CNN: 15.21 seconds, Faster R-CNN: 15.27 seconds\n",
            "CNN Accuracy: 99.01%, CNN F1 Score: 0.9901\n",
            "Faster R-CNN Accuracy: 99.21%, Faster R-CNN F1 Score: 0.9921\n",
            "Epoch [9/10], CNN Loss: 0.0113, Faster R-CNN Loss: 0.0069\n",
            "Training Time - CNN: 15.10 seconds, Faster R-CNN: 21.37 seconds\n",
            "CNN Accuracy: 99.10%, CNN F1 Score: 0.9910\n",
            "Faster R-CNN Accuracy: 98.98%, Faster R-CNN F1 Score: 0.9898\n",
            "Epoch [10/10], CNN Loss: 0.0103, Faster R-CNN Loss: 0.0069\n",
            "Training Time - CNN: 15.34 seconds, Faster R-CNN: 15.48 seconds\n",
            "CNN Accuracy: 99.11%, CNN F1 Score: 0.9911\n",
            "Faster R-CNN Accuracy: 99.09%, Faster R-CNN F1 Score: 0.9909\n",
            "-----------\n",
            "CNN Accuracy: 0.9911\n",
            "Faster RCNN Accuracy: 0.9909\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import time\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Function to calculate F1 score\n",
        "def calculate_f1_score(y_true, y_pred):\n",
        "    return f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Training function\n",
        "def train_model(model, criterion, optimizer, train_loader, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()  # Start time for epoch\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    end_time = time.time()  # End time for epoch\n",
        "    return running_loss / len(train_loader), end_time - start_time  # Return loss and training time\n",
        "\n",
        "# Testing function\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    return accuracy_score(y_true, y_pred), calculate_f1_score(y_true, y_pred)\n",
        "\n",
        "# Inside your evaluation function, move input tensors to the same device\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to device\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Initialize models\n",
        "cnn_model = CNN().to(device)\n",
        "faster_rcnn_model = FasterRCNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate)\n",
        "faster_rcnn_optimizer = optim.Adam(faster_rcnn_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training and evaluation\n",
        "for epoch in range(num_epochs):\n",
        "    cnn_loss, cnn_time = train_model(cnn_model, criterion, cnn_optimizer, train_loader, device)\n",
        "    faster_rcnn_loss, faster_rcnn_time = train_model(faster_rcnn_model, criterion, faster_rcnn_optimizer, train_loader, device)\n",
        "    cnn_accuracy, cnn_f1_score = test_model(cnn_model, test_loader, device)\n",
        "    faster_rcnn_accuracy, faster_rcnn_f1_score = test_model(faster_rcnn_model, test_loader, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], CNN Loss: {cnn_loss:.4f}, Faster R-CNN Loss: {faster_rcnn_loss:.4f}\")\n",
        "    print(f\"Training Time - CNN: {cnn_time:.2f} seconds, Faster R-CNN: {faster_rcnn_time:.2f} seconds\")\n",
        "    print(f\"CNN Accuracy: {cnn_accuracy*100:.2f}%, CNN F1 Score: {cnn_f1_score:.4f}\")\n",
        "    print(f\"Faster R-CNN Accuracy: {faster_rcnn_accuracy*100:.2f}%, Faster R-CNN F1 Score: {faster_rcnn_f1_score:.4f}\")\n",
        "\n",
        "# Ensure model and input tensors are on the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"-----------\")\n",
        "print(\"CNN Accuracy:\", evaluate(cnn_model.to(device),test_loader,device))\n",
        "print(\"Faster RCNN Accuracy:\", evaluate(faster_rcnn_model.to(device),test_loader,device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16LslZshdwY"
      },
      "source": [
        "## 4. By using retrained models (VGG16 and AlexNet) fine tune your model to the new dataSet,then compare the obtained results to CNN and Faster R-CNN, what is your conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "McpIzyZ5hhKw",
        "outputId": "1ec37d0c-19d3-4de7-dcac-d247a6494151"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:04<00:00, 129MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:03<00:00, 73.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 111696007.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 72492934.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31274097.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11817945.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-94a201472b60>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0malexnet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malexnet_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malexnet_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mvgg16_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg16_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg16_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0malexnet_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malexnet_f1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained models\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "\n",
        "# Modify classifier layers\n",
        "num_classes = 10  # MNIST has 10 classes\n",
        "vgg16.classifier[6] = nn.Linear(4096, num_classes)\n",
        "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "# Convert grayscale images to RGB format\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to fit VGG16 and AlexNet input size\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "vgg16_optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "alexnet_optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Move models to device\n",
        "alexnet = alexnet.to(device)\n",
        "vgg16 = vgg16.to(device)\n",
        "\n",
        "# Training and evaluation\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet_loss, alexnet_time = train_model(alexnet, criterion, alexnet_optimizer, train_loader, device)\n",
        "    vgg16_loss, vgg16_time = train_model(vgg16, criterion, vgg16_optimizer, train_loader, device)\n",
        "    alexnet_accuracy, alexnet_f1_score = test_model(alexnet, test_loader, device)\n",
        "    vgg16_accuracy, vgg16_f1_score = test_model(vgg16, test_loader, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], AlexNet Loss: {alexnet_loss:.4f}, VGG16 Loss: {vgg16_loss:.4f}\")\n",
        "    print(f\"Training Time - AlexNet: {alexnet_time:.2f} seconds, VGG16: {vgg16_time:.2f} seconds\")\n",
        "    print(f\"AlexNet Accuracy: {alexnet_accuracy*100:.2f}%, AlexNet F1 Score: {alexnet_f1_score:.4f}\")\n",
        "    print(f\"VGG16 Accuracy: {vgg16_accuracy*100:.2f}%, VGG16 F1 Score: {vgg16_f1_score:.4f}\")\n",
        "\n",
        "print(\"-----------\")\n",
        "print(\"VGG16 Accuracy:\", evaluate(vgg16, test_loader))\n",
        "print(\"AlexNet Accuracy:\", evaluate(alexnet, test_loader))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
